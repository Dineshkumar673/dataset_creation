# -*- coding: utf-8 -*-
"""1_LSTM_grid_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b5jb2cYiX0aZ1oNi4sv4LGAoHAjjFt0m
"""

#Importing Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
dd=pd.read_csv('/mnt/fs/var_kp_2000.csv')
X=dd.iloc[:,1:].values
y=dd['class_name']
le = LabelEncoder()
#train['class'] = le.fit_transform(train['class'])
y = le.fit_transform(y)
# one hot encode output variable
from tensorflow.keras.utils import to_categorical
y = to_categorical(y)
X=np.reshape(X,(100769,975,1))
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
# Feature Scaling
#from sklearn.preprocessing import MinMaxScaler
#sc = MinMaxScaler()
#X_train = np.reshape(X_train,(-1,1))
#y_train = np.reshape(y_train,(-1,1))
#X_train = sc.fit_transform(X_train)
#y_train = sc.fit_transform(y_train)
#X_test =sc.fit_transform(X_test)
#y_test = np.reshape(y_test,(-1,1))
#y_test = sc.fit_transform(y_test)

# Importing the Keras libraries and packages for LSTM
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler   
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import activations
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Dense,Dropout,LSTM,BatchNormalization
from keras.callbacks import EarlyStopping
#import tensorflowjs as tfjs
from tensorflow.keras.callbacks import  ModelCheckpoint

model2 = tf.keras.models.Sequential([
     tf.keras.layers.LSTM(units =40,input_shape = X.shape[1:],return_sequences=True),
     tf.keras.layers.BatchNormalization(),
     tf.keras.layers.LSTM(units =50),
     tf.keras.layers.BatchNormalization(),
     tf.keras.layers.Dense(40, activation='relu'),
     tf.keras.layers.Dropout(0.4),
     tf.keras.layers.Dense(34),
     tf.keras.layers.Activation(activations.softmax)

])
print(model2.summary())

# Compiling the RNN
model2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])

from keras.callbacks import CSVLogger

# checkpoint
checkpoint_path_1='"/mnt/fs/h5_files/model1/model_history_log_1.csv"'
checkpoint_path="/mnt/fs/h5_files/model1/model.{epoch:02d}-{val_accuracy:.2f}.h5"
keras_callbacks   = [
      EarlyStopping(monitor='val_loss', patience=3, mode='min', min_delta=0.0001),
      ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max'),
      CSVLogger(checkpoint_path_1,separator=",", append=True)

]

# Fitting the RNN to the Training set
model2.fit(X_train, y_train,validation_data=(X_test, y_test), epochs = 150, batch_size = 32,verbose=1,callbacks=keras_callbacks)

train_accuracy1=model2.evaluate(X_train,y_train,verbose=0)
print(train_accuracy1[1])

test_accuracy1=model2.evaluate(X_test,y_test,verbose=0)
print(test_accuracy1[1])
